{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=20)  # Fully-featured local Dask cluster\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "long_short_name_dict = {\"2m_temperature\": \"t2m\"}\n",
    "\n",
    "\n",
    "def gen_file_list(\n",
    "    variable: str,\n",
    "    start_datetime: str,\n",
    "    end_datetime: str,\n",
    "    time_resolution: str,  # e.g., \"hour\", \"day\", \"month\", \"year\"\n",
    "    time_agg_method: str,  # e.g., \"mean\", \"max\", \"min\"\n",
    "):\n",
    "    file_list = []\n",
    "    if time_resolution == \"hour\":\n",
    "        start_year = start_datetime[:4]\n",
    "        end_year = end_datetime[:4]\n",
    "        for year in range(int(start_year), int(end_year) + 1):\n",
    "            file_path = f\"/data/era5/raw/{variable}/{variable}-{year}.nc\"\n",
    "            file_list.append(file_path)\n",
    "    else:\n",
    "        file_path = (\n",
    "            f\"/home/huan1531/iharp-quick-aggregate/data/output/{variable}-{time_resolution}-{time_agg_method}.nc\"\n",
    "        )\n",
    "        file_list.append(file_path)\n",
    "    print(file_list)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = \"2m_temperature\"\n",
    "start_datetime = \"2000-01-01 00:00:00\"\n",
    "end_datetime = \"2023-12-31 23:00:00\"\n",
    "# # 20% space\n",
    "# max_lat = 80\n",
    "# min_lat = -13\n",
    "# min_lon = 0\n",
    "# max_lon = 140\n",
    "# # 10% space\n",
    "# max_lat = 81\n",
    "# min_lat = 0\n",
    "# min_lon = 0\n",
    "# max_lon = 80\n",
    "# # Greenland\n",
    "# max_lat = 85\n",
    "# min_lat = 60\n",
    "# min_lon = -70\n",
    "# max_lon = -10\n",
    "# Alaska\n",
    "max_lat = 75\n",
    "min_lat = 50\n",
    "min_lon = -170\n",
    "max_lon = -140\n",
    "print((max_lat - min_lat) / 180 * (max_lon - min_lon) / 360)\n",
    "\n",
    "time_resolution = \"hour\"\n",
    "time_agg_method = \"mean\"\n",
    "time_series_aggregation_method = \"mean\"\n",
    "\n",
    "\n",
    "file_list = gen_file_list(variable, start_datetime, end_datetime, time_resolution, time_agg_method)\n",
    "ds = xr.open_mfdataset(file_list, engine=\"netcdf4\", parallel=True, chunks={\"time\": 2000})\n",
    "ds = ds.sel(\n",
    "    time=slice(start_datetime, end_datetime),\n",
    "    latitude=slice(max_lat, min_lat),\n",
    "    longitude=slice(min_lon, max_lon),\n",
    ")\n",
    "if time_series_aggregation_method == \"mean\":\n",
    "    ts = ds.mean(dim=[\"latitude\", \"longitude\"])\n",
    "elif time_series_aggregation_method == \"max\":\n",
    "    ts = ds.max(dim=[\"latitude\", \"longitude\"])\n",
    "elif time_series_aggregation_method == \"min\":\n",
    "    ts = ds.min(dim=[\"latitude\", \"longitude\"])\n",
    "ts.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
